{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiments with dataset in paper,\n",
    "Explainable Verbal Deception Detection using Transformers\n",
    "Loukas Ilias, Felix Soldner and Bennett Kleinberg\n",
    "uses LIWC-15\n",
    "\"\"\"\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # source the ~/.zshrc file\n",
    "\n",
    "# https://platform.openai.com/docs/guides/rate-limits/error-mitigation\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "# CONSTANTS, until we change them ;-)\n",
    "new_line = '\\n'\n",
    "nb_test_samples = 10\n",
    "nb_few_shot_samples_of_each_class = 3 # truth and deception\n",
    "delimiter = '```\\n'\n",
    "# MODEL = \"gpt-4\"\n",
    "MODEL = \"gpt-4-1106-preview\" # input tokes 3x cheaper and output tokens 2x cheaper than GPT-4\n",
    "temperature = 0.7\n",
    "SEED = 12345"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1640, 101)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv ('dataset/LIWC-15 Results - sign_events_data_statements - LIWC Analysis.csv')\n",
    "# simple EDA\n",
    "# print(df)\n",
    "# print(df.columns)\n",
    "print(f'shape: {df.shape}')  # should be 1640 x 6\n",
    "# df.head\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the LIWC markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1640, 101)\n",
      "Index(['signevent', 'q1', 'q2', 'unid', 'id', 'outcome_class', 'Segment', 'WC',\n",
      "       'Analytic', 'Clout',\n",
      "       ...\n",
      "       'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro',\n",
      "       'Parenth', 'OtherP', 'Emoji'],\n",
      "      dtype='object', length=101)\n",
      "['AllPunc']\n",
      "['Analytic', 'Apostro', 'Authentic', 'Clout', 'Colon', 'Comma', 'Dash', 'Dic', 'Emoji', 'Exclam']\n",
      "['OtherP', 'Parenth', 'Period', 'QMark', 'Quote', 'Segment', 'SemiC', 'Sixltr', 'Tone', 'WC']\n",
      "['WPS', 'achieve', 'adj', 'adverb', 'affect', 'affiliation', 'anger', 'anx', 'article', 'assent']\n",
      "['auxverb', 'bio', 'body', 'cause', 'certain', 'cogproc', 'compare', 'conj', 'death', 'differ']\n",
      "['discrep', 'drives', 'family', 'feel', 'female', 'filler', 'focusfuture', 'focuspast', 'focuspresent', 'friend']\n",
      "['function', 'health', 'hear', 'home', 'i', 'id', 'informal', 'ingest', 'insight', 'interrog']\n",
      "['ipron', 'leisure', 'male', 'money', 'motion', 'negate', 'negemo', 'netspeak', 'nonflu', 'number']\n",
      "['outcome_class', 'percept', 'posemo', 'power', 'ppron', 'prep', 'pronoun', 'q1', 'q2', 'quant']\n",
      "['relativ', 'relig', 'reward', 'risk', 'sad', 'see', 'sexual', 'shehe', 'signevent', 'social']\n",
      "['space', 'swear', 'tentat', 'they', 'time', 'unid', 'verb', 'we', 'work', 'you']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "liwc_15 = pd.read_csv ('dataset/LIWC-15 Results - sign_events_data_statements - LIWC Analysis.csv')\n",
    "# simple EDA\n",
    "\n",
    "print(f'shape: {liwc_15.shape}')  # should be 1640, \n",
    "print(liwc_15.columns)\n",
    "cols = sorted(liwc_15.columns)\n",
    "nb_attrib_per_line = 10\n",
    "print_buf = []\n",
    "for i, attrib in enumerate(cols):\n",
    "    print_buf.append(attrib)\n",
    "    if i % nb_attrib_per_line == 0:\n",
    "        print(print_buf)\n",
    "        print_buf = []\n",
    "\n",
    "truth_markers = df[['ingest', 'bio', 'Analytic', 'number', 'leisure', 'focusfuture']]\n",
    "deception_markers = df[['Apostro', 'focuspast', 'reward', 'WC', 'pronoun', 'ppron', 'Exclam', 'Tone']]\n",
    "liwc_markers = df[['ingest', 'bio', 'Analytic', 'number', 'leisure', 'Apostro', 'focuspast', 'reward', 'WC', 'pronoun']]\n",
    "# print(truth_markers)\n",
    "# print(deception_markers)\n",
    "# print(liwc_markers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some quick test to see how the truth/deceit markers are bahaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signevent                     attend a summer bbq with friends\n",
      "q1           I will attend the party at 1pm at my friends c...\n",
      "q2           I have been invited to this party over 2 month...\n",
      "unid                                                  Lj633672\n",
      "id                                                         101\n",
      "                                   ...                        \n",
      "Quote                                                      0.0\n",
      "Apostro                                                    0.0\n",
      "Parenth                                                    0.0\n",
      "OtherP                                                     0.0\n",
      "Emoji                                                      0.0\n",
      "Name: 100, Length: 101, dtype: object\n",
      "Type: <class 'str'>\n",
      "{\"truthful attributes\": {\"ingestion\": 5, \"biological processes\": 6, \"analytic reasoning\": 60, \"numbers\": 3, \"leisure\": 5, \"future focus\": 5}, \"deceptive attributes\": {\"apostrophes\": 0, \"past focus\": 3, \"reward\": 3, \"word count\": 58, \"all pronouns\": 12, \"personal pronouns\": 8, \"exclamation marks\": 0, \"emotional tone\": 85}}\n",
      "outcome: t\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import json\n",
    "# TODO see if this can be more friendlier if GPT does not know about LIWC\n",
    "def construct_liwc_attributes_json(row):\n",
    "    # print('class:', 'truthful' if df.iloc[row]['outcome_class'] == 't' else 'deceptive')\n",
    "    # print(liwc_markers.iloc[row])\n",
    "    # print(f'q1:\\n {textwrap.fill(df.iloc[row][\"q1\"], 100)}')\n",
    "    # print(f'q2:\\n {textwrap.fill(df.iloc[row][\"q2\"], 100)}')\n",
    "    \n",
    "    truthful_attributes = ['ingest', 'bio', 'Analytic', 'number', 'leisure', 'focusfuture']\n",
    "    deceptive_attributes = ['Apostro', 'focuspast', 'reward', 'WC', 'pronoun', 'ppron', 'Exclam', 'Tone']\n",
    "    \n",
    "    friendly_truthful_attribs = ['ingestion', 'biological processes', 'analytic reasoning', \n",
    "                                 'numbers', 'leisure', 'future focus']\n",
    "    friendly_deceptive_attribs = ['apostrophes', 'past focus', 'reward', 'word count', \n",
    "                                  'all pronouns', 'personal pronouns', 'exclamation marks', 'emotional tone']\n",
    "                  \n",
    "    # attributes = ['ingest', 'bio', 'Analytic', 'number', 'leisure', 'Apostro', 'focuspast', 'reward', 'WC', 'pronoun']\n",
    "    truthful_data = {}\n",
    "    for truthful_attribute, friendly_truthful_attrib in zip(truthful_attributes, friendly_truthful_attribs):\n",
    "        # data[attribute] = str(row[attribute])\n",
    "        truthful_data[friendly_truthful_attrib] = int(row[truthful_attribute]) \n",
    "    deceptive_data = {}\n",
    "    for deceptive_attribute, friendly_deceptive_attrib in zip(deceptive_attributes, friendly_deceptive_attribs):\n",
    "        # data[attribute] = str(row[attribute])\n",
    "        deceptive_data[friendly_deceptive_attrib] = int(row[deceptive_attribute]) \n",
    "        \n",
    "    \n",
    "    # return_str = json.dumps({\"truthful attributes\": json.dumps(truthful_data), \"deceptive attributes\": json.dumps(deceptive_data)})\n",
    "    return json.dumps({\"truthful attributes\": truthful_data, \"deceptive attributes\": deceptive_data})\n",
    "\n",
    "# try it out\n",
    "try_on_row = 100\n",
    "print(df.iloc[try_on_row])\n",
    "truth_deception_json = construct_liwc_attributes_json(df.iloc[try_on_row].copy())\n",
    "print(f'Type: {type(truth_deception_json)}')\n",
    "# print(f\"truth attributes: {truth_deception_json['truthful attributes']}\")\n",
    "# print(f\"deceptive attributes: {truth_deception_json['deceptive attributes']}\")\n",
    "print(truth_deception_json)\n",
    "print(f\"outcome: {df.iloc[try_on_row]['outcome_class']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a random list of indices (from both classes) to be used for k-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth df shape: (783, 101)\n",
      "deceit df shape: (857, 101)\n",
      "non-repeating random numbers are:\n",
      "truth indices:\" [746, 158, 563]\n",
      "non-repeating random numbers are:\n",
      "deceit indices: [1222, 1566, 1608]\n",
      "truth + deceit indices for(few-shot-list)\n",
      " [746, 158, 563, 1222, 1566, 1608]\n",
      "[746, 1222, 158, 1566, 563, 1608]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20476/2622560624.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  truth_df['outcome_class'] = df['outcome_class'].replace('t','truthful')\n",
      "/tmp/ipykernel_20476/2622560624.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deceit_df['outcome_class'] = df['outcome_class'].replace('d','deceptive')\n"
     ]
    }
   ],
   "source": [
    "def filter_by_class(df, category):\n",
    "   return df[df['outcome_class']== category]\n",
    "\n",
    "truth_df = filter_by_class(df, 't')\n",
    "# print(truth_df)\n",
    "print(f'truth df shape: {truth_df.shape}')  # should be 1640 x 6\n",
    "\n",
    "# replace with a more expressive word, truthful\n",
    "truth_df['outcome_class'] = df['outcome_class'].replace('t','truthful')\n",
    "# print(truth_df)\n",
    "\n",
    "deceit_df = filter_by_class(df, 'd')\n",
    "# print(deceit_df)\n",
    "print(f'deceit df shape: {deceit_df.shape}')  # should be 1640 x 6\n",
    "\n",
    "# replace with a more expressive word, deceitful\n",
    "deceit_df['outcome_class'] = df['outcome_class'].replace('d','deceptive')\n",
    "# print(deceit_df)\n",
    "\n",
    "# pick random non-repeating rows\n",
    "def pick_randon_non_repeating(df, quantity):\n",
    "    import random\n",
    "    rand_df = pd.DataFrame()\n",
    "    random_list = random.sample(range(df.shape[0]), quantity)\n",
    "    print(\"non-repeating random numbers are:\")\n",
    "    return df.iloc[random_list], random_list\n",
    "\n",
    "random_truth_df, truth_indices_list = pick_randon_non_repeating(truth_df, nb_few_shot_samples_of_each_class)\n",
    "# print(f'random truth list:\\n {random_truth_df}')\n",
    "print(f'truth indices:\" {truth_indices_list}')\n",
    "\n",
    "random_deceit_df, deceit_indices_list = pick_randon_non_repeating(deceit_df, nb_few_shot_samples_of_each_class)\n",
    "# print(f'random deceit list:\\n {random_deceit_df}')\n",
    "deceit_indices_list = [x + truth_df.shape[0] for x in deceit_indices_list] # do this to exclude from original list\n",
    "print(f'deceit indices: {deceit_indices_list}')\n",
    "\n",
    "random_truth_deceit_df = pd.concat([random_truth_df, random_deceit_df])\n",
    "\n",
    "few_shot_list = truth_indices_list + deceit_indices_list\n",
    "print(f'truth + deceit indices for(few-shot-list)\\n {few_shot_list}')\n",
    "alternating_indices = []\n",
    "for i, j in zip(truth_indices_list, deceit_indices_list):\n",
    "    alternating_indices.append(i)\n",
    "    alternating_indices.append(j)\n",
    "print(alternating_indices)\n",
    "random_truth_deceit_df = df.iloc[alternating_indices, :]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the OpenAI call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(prompt, model, temperature):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        seed=SEED,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=3), stop=stop_after_attempt(3))\n",
    "def get_chat_completion_with_backoff(prompt, model, temperature):\n",
    "    return get_chat_completion(prompt, model, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = f\"\"\"\n",
    "You are tasked to classify the response to questions into two classes: truthful or deceptive.\n",
    "You'll be presented with the following pieces of information on an activity:\n",
    "(1) The title of the activity.\n",
    "(2) An answer to the question: Please describe your activity. Be as specific as possible.\n",
    "(3) An answer to the follow-on question: What information can you give us to reassure us that you are telling the truth?\n",
    "(4) A Linguistic Inquiry Word Count (LIWC) attributes in two attrubute groups in the JSON format: \n",
    "\"truthful attributes\" in decreasing order of importance and \n",
    "\"deceptive attributes\" in decreasing order of importance.\n",
    "Think step by step.\n",
    "Using all of the above information, complete the response. which must be either 'truthful' or 'deceptive' and nothing else.\n",
    "\n",
    "Here are a few examples delimited by triple backticks:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response_1_heading = \"\"\"Response #1:\\n\"\"\"\n",
    "response_2_heading = \"\"\"Response #2:\\n\"\"\"\n",
    "\n",
    "liwc_header = \"\"\"Combined Linguistic Inquiry Word Count (LIWC) of activity decription and assurance description as key/value pairs in the JSON format:\\n\"\"\"\n",
    "\n",
    "def double_quoted(s):\n",
    "    return '\"'+ s + '\"'\n",
    "\n",
    "def construct_activity_scenario(row):\n",
    "    # activity_header = 'Title of the Activity: ' + new_line\n",
    "    activity_header = 'Activity: ' \n",
    "    # activity_description_header = 'Question #1: Please describe your activity. Be as specific as possible.'\n",
    "    activity_description_header = 'Activity Description: '\n",
    "\n",
    "    # activity_reassurance_header = 'Question #2: What information can you give us to reassure us that you are telling the truth?'\n",
    "    activity_reassurance_header = 'Assurance the activity description is truthful: '\n",
    "\n",
    "    activity = activity_header + row['signevent'] + 2* new_line\n",
    "    # q1_r1 = activity_description_header + new_line + response_1_heading + row['q1'] + 2 * new_line\n",
    "    # q2_r2 = activity_reassurance_header + new_line + response_2_heading + row['q2'] + 2 * new_line\n",
    "\n",
    "    q1_r1 = activity_description_header + double_quoted(row['q1']) + 2 * new_line\n",
    "    q2_r2 = activity_reassurance_header + double_quoted(row['q2']) + 2 * new_line\n",
    "\n",
    "    return activity + q1_r1 + q2_r2\n",
    "\n",
    "def construct_outcome(row):\n",
    "    outcome = \"Are the activity and assurance description truthful or deceptive?\\n\"\n",
    "    return outcome  + row['outcome_class'] + new_line\n",
    "\n",
    "def construct_liwc_json(row):\n",
    "    pass\n",
    "\n",
    "def construct_few_shot_prompt(few_shot_df, infer_row):\n",
    "    # constructed as a list\n",
    "    prompt = []\n",
    "    prompt.append(intro)\n",
    "    \n",
    "    for _, row in few_shot_df.iterrows():\n",
    "        prompt.append(delimiter)\n",
    "        prompt.append(construct_activity_scenario(row))\n",
    "        prompt.append(liwc_header)\n",
    "        prompt.append(construct_liwc_attributes_json(row))\n",
    "        prompt.append(2 * new_line)\n",
    "        prompt.append(construct_outcome(row))\n",
    "        prompt.append(delimiter)\n",
    "        prompt.append(new_line * 2)    \n",
    "\n",
    "    prompt.append(delimiter)\n",
    "    prompt.append(construct_activity_scenario(infer_row))\n",
    "    prompt.append(liwc_header)\n",
    "    prompt.append(construct_liwc_attributes_json(infer_row))\n",
    "    prompt.append(2 * new_line) \n",
    "    prompt.append(construct_outcome(infer_row)) # has to have a blank outcome to be filled by the LLM\n",
    "    prompt.append(delimiter)\n",
    "\n",
    "\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_indices(df, total, exclude_list):\n",
    "    import random\n",
    "    rand_list = []\n",
    "    count = 0\n",
    "    print('shape of df:', df.shape[0])\n",
    "    print('exclude list:', exclude_list)\n",
    "    while count < total:\n",
    "        rand_row = random.randrange(df.shape[0])\n",
    "        if rand_row not in exclude_list:\n",
    "            rand_list.append(rand_row)\n",
    "            count += 1\n",
    "    return rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df: 1640\n",
      "exclude list: [746, 158, 563, 1222, 1566, 1608]\n",
      "Indices to test: [634, 419, 468, 446, 798, 1372, 47, 100, 811, 1028]\n",
      "counts -  truthful: 6 deceptive: 4\n"
     ]
    }
   ],
   "source": [
    "test_indices = create_test_indices(df, nb_test_samples, few_shot_list)  # exclude the ones in the few shot list\n",
    "print(f'Indices to test: {test_indices}')\n",
    "truthful_count = 0\n",
    "deceptive_count = 0\n",
    "for test_index in test_indices:\n",
    "    if df.loc[test_index]['outcome_class'] == 't':\n",
    "        truthful_count +=1\n",
    "    elif df.loc[test_index]['outcome_class'] == 'd':\n",
    "        deceptive_count += 1\n",
    "    else:\n",
    "        print('error')\n",
    "print(f'counts -  truthful: {truthful_count} deceptive: {deceptive_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: INDEX: 634 GROUND TRUTH: truthful, PREDICTED: deceptive - wrong\n",
      "#1: INDEX: 419 GROUND TRUTH: truthful, PREDICTED: deceptive - wrong\n",
      "#2: INDEX: 468 GROUND TRUTH: truthful, PREDICTED: deceptive - wrong\n",
      "#3: INDEX: 446 GROUND TRUTH: truthful, PREDICTED: truthful - correct\n",
      "#4: INDEX: 798 GROUND TRUTH: deceptive, PREDICTED: truthful - wrong\n",
      "#5: INDEX: 1372 GROUND TRUTH: deceptive, PREDICTED: deceptive - correct\n",
      "#6: INDEX: 47 GROUND TRUTH: truthful, PREDICTED: truthful - correct\n",
      "#7: INDEX: 100 GROUND TRUTH: truthful, PREDICTED: truthful - correct\n",
      "#8: INDEX: 811 GROUND TRUTH: deceptive, PREDICTED: deceptive - correct\n",
      "#9: INDEX: 1028 GROUND TRUTH: deceptive, PREDICTED: deceptive - correct\n"
     ]
    }
   ],
   "source": [
    "y_ground_truth = []  # for computing F1-score\n",
    "y_predicted = []\n",
    "\n",
    "for i, index in enumerate(test_indices):\n",
    "    infer_row = df.loc[index].copy()\n",
    "    # print(f'Inferring the `class_outcome` for:\\n{infer_row}')\n",
    "    ground_truth = 'truthful' if infer_row['outcome_class'] == 't' else 'deceptive'\n",
    "    # mask the `outcome_class` field since you want to predict it\n",
    "    infer_row['outcome_class'] = ''\n",
    "\n",
    "    # print(f'Original\\n:{df.loc[index]}')\n",
    "    # print(f'infer row\\n: {infer_row}')\n",
    "\n",
    "    prompt = construct_few_shot_prompt(random_truth_deceit_df, infer_row)\n",
    "    # print(prompt)\n",
    "    prompt = ''.join(prompt)\n",
    "    \n",
    "    # print(f'Prompt:\\n{prompt}')\n",
    "\n",
    "    response = get_chat_completion_with_backoff(\n",
    "        prompt=prompt,\n",
    "        model=MODEL,\n",
    "        temperature=temperature\n",
    "    )    \n",
    "        \n",
    "    print(f'#{i}: INDEX: {index} GROUND TRUTH: {ground_truth}, PREDICTED: {response} - {\"wrong\" if ground_truth != response else \"correct\"}')\n",
    "    y_ground_truth.append(ground_truth)\n",
    "    y_predicted.append(response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples predicted: 10\n",
      "GT: deceptive count:4 truthful count: 6\n",
      "PRED: deceptive count:6 truthful count: 4\n",
      "GT  : ['truthful', 'truthful', 'truthful', 'truthful', 'deceptive', 'deceptive', 'truthful', 'truthful', 'deceptive', 'deceptive']\n",
      "PRED: ['deceptive', 'deceptive', 'deceptive', 'truthful', 'truthful', 'deceptive', 'truthful', 'truthful', 'deceptive', 'deceptive']\n",
      "TN, FP, FN, TP: [3 1 3 3]\n",
      "Weighted F1-score: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print(f'Total samples predicted: {len(y_predicted)}')\n",
    "print(f\"GT: deceptive count:{y_ground_truth.count('deceptive')} truthful count: {y_ground_truth.count('truthful')}\")\n",
    "print(f\"PRED: deceptive count:{y_predicted.count('deceptive')} truthful count: {y_predicted.count('truthful')}\")\n",
    "print(f'GT  : {y_ground_truth}\\nPRED: {y_predicted}')\n",
    "print(f'TN, FP, FN, TP: {confusion_matrix(y_ground_truth, y_predicted).ravel()}')\n",
    "print(f\"Weighted F1-score: {f1_score(y_ground_truth, y_predicted, average='weighted'):0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyagi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
