{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiments with dataset in paper,\n",
    "Explainable Verbal Deception Detection using Transformers\n",
    "Loukas Ilias, Felix Soldner and Bennett Kleinberg\n",
    "uses LIWC-15\n",
    "\"\"\"\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # source the ~/.zshrc file\n",
    "\n",
    "# https://platform.openai.com/docs/guides/rate-limits/error-mitigation\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "# constants, until you change them ;-)\n",
    "new_line = '\\n'\n",
    "nb_test_samples = 10\n",
    "nb_few_shot_samples_of_each_class =3 # truth and deception\n",
    "delimiter = '```\\n'\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "# MODEL = \"text-davinci-003\"\n",
    "MODEL = \"gpt-4\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv ('LIWC-15 Results - sign_events_data_statements - LIWC Analysis')\n",
    "# simple EDA\n",
    "# print(df)\n",
    "# print(df.columns)\n",
    "print(f'shape: {df.shape}')  # should be 1640 x 6\n",
    "df.head\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the LIWC markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "liwc_15 = pd.read_csv ('LIWC-15 Results - sign_events_data_statements - LIWC Analysis')\n",
    "# simple EDA\n",
    "\n",
    "print(f'shape: {liwc_15.shape}')  # should be 1640, \n",
    "print(liwc_15.columns)\n",
    "cols = sorted(liwc_15.columns)\n",
    "nb_attrib_per_line = 10\n",
    "print_buf = []\n",
    "for i, attrib in enumerate(cols):\n",
    "    print_buf.append(attrib)\n",
    "    if i % nb_attrib_per_line == 0:\n",
    "        print(print_buf)\n",
    "        print_buf = []\n",
    "\n",
    "truth_markers = df[['ingest', 'bio', 'Analytic', 'number', 'leisure', 'focusfuture']]\n",
    "deception_markers = df[['Apostro', 'focuspast', 'reward', 'WC', 'pronoun', 'ppron', 'Exclam', 'Tone']]\n",
    "liwc_markers = df[['ingest', 'bio', 'Analytic', 'number', 'leisure', 'Apostro', 'focuspast', 'reward', 'WC', 'pronoun']]\n",
    "# print(truth_markers)\n",
    "# print(deception_markers)\n",
    "print(liwc_markers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some quick test to see how the truth/deceit markers are bahaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import json\n",
    "# TODO see if this can be more friendlier if GPT does not know about LIWC\n",
    "def construct_liwc_attributes_json(row):\n",
    "    # print('class:', 'truthful' if df.iloc[row]['outcome_class'] == 't' else 'deceptive')\n",
    "    # print(liwc_markers.iloc[row])\n",
    "    # print(f'q1:\\n {textwrap.fill(df.iloc[row][\"q1\"], 100)}')\n",
    "    # print(f'q2:\\n {textwrap.fill(df.iloc[row][\"q2\"], 100)}')\n",
    "    \n",
    "    attributes = ['ingest', 'bio', 'Analytic', 'number', 'leisure', 'Apostro', 'focuspast', 'reward', 'WC', 'pronoun']\n",
    "    data = {}\n",
    "    for attribute in attributes:\n",
    "        data[attribute] = str(row[attribute])\n",
    "    \n",
    "    return json.dumps(data)\n",
    "\n",
    "print(df.iloc[0])\n",
    "print(construct_liwc_attributes_json(df.iloc[0].copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_class(df, category):\n",
    "   return df[df['outcome_class']== category]\n",
    "\n",
    "truth_df = filter_by_class(df, 't')\n",
    "# print(truth_df)\n",
    "print(f'truth df shape: {truth_df.shape}')  # should be 1640 x 6\n",
    "\n",
    "# replace with a more expressive word, truthful\n",
    "truth_df['outcome_class'] = df['outcome_class'].replace('t','truthful')\n",
    "print(truth_df)\n",
    "\n",
    "deceit_df = filter_by_class(df, 'd')\n",
    "# print(deceit_df)\n",
    "print(f'deceit df shape: {deceit_df.shape}')  # should be 1640 x 6\n",
    "\n",
    "# replace with a more expressive word, deceitful\n",
    "deceit_df['outcome_class'] = df['outcome_class'].replace('d','deceptive')\n",
    "print(deceit_df)\n",
    "\n",
    "# pick random non-repeating rows\n",
    "def pick_randon_non_repeating(df, quantity):\n",
    "    import random\n",
    "    rand_df = pd.DataFrame()\n",
    "    random_list = random.sample(range(df.shape[0]), quantity)\n",
    "    print(\"non-repeating random numbers are:\")\n",
    "    return df.iloc[random_list], random_list\n",
    "\n",
    "random_truth_df, truth_indices_list = pick_randon_non_repeating(truth_df, nb_few_shot_samples_of_each_class)\n",
    "print(f'random truth list:\\n, {random_truth_df}')\n",
    "print(f'truth indices:\" {truth_indices_list}')\n",
    "\n",
    "random_deceit_df, deceit_indices_list = pick_randon_non_repeating(deceit_df, nb_few_shot_samples_of_each_class)\n",
    "print(f'random deceit list:\\n, {random_deceit_df}')\n",
    "deceit_indices_list = [x + truth_df.shape[0] for x in deceit_indices_list] # do this to exclude from poriginal list\n",
    "print(f'deceit indices: {deceit_indices_list}')\n",
    "\n",
    "random_truth_deceit_df = pd.concat([random_truth_df, random_deceit_df])\n",
    "few_shot_list = truth_indices_list + deceit_indices_list\n",
    "print(f'truth + deceit indices\" {few_shot_list}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the OpenAI call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_chat_completion_with_backoff(prompt, model=\"gpt-3.5-turbe\"):\n",
    "    return get_chat_completion(prompt, model)\n",
    "\n",
    "def get_completion(prompt, model='text-davinci-003'):\n",
    "    response = openai.Completion.create(\n",
    "        prompt=prompt,\n",
    "        model=model\n",
    "    )\n",
    "    return response.choices[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = f\"\"\"\n",
    "You are tasked to classify the response to questions into two classes: truthful or deceptive.\n",
    "You'll be presented with the following pieces of information on an activity:\n",
    "(1) The title of the activity.\n",
    "(2) An answwer to a question: Please describe your activity. Be as specific as possible.\n",
    "(3) A answer to a follow-on question: What information can you give us to reassure us that you are telling the truth?\n",
    "(4) A Linguistic Inquiry Word Count (LIWC) category values as JSON data\n",
    "Using all four pieces of information, complete the response with either 'truthful' or 'deceptive'.\n",
    "\n",
    "Here are a few examples delimited by triple backticks:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response_1_heading = \"\"\"Response #1:\\n\"\"\"\n",
    "response_2_heading = \"\"\"Response #2:\\n\"\"\"\n",
    "\n",
    "liwc_header = \"\"\"Linguistic Inquiry Word Count (LIWC):\\n\"\"\"\n",
    "\n",
    "def construct_activity_scenario(row):\n",
    "    # activity_header = 'Title of the Activity: ' + new_line\n",
    "    activity_header = 'Activity: ' + new_line\n",
    "    activity_description_header = 'Question #1: \\nPlease describe your activity. Be as specific as possible.'\n",
    "\n",
    "    activity_reassurance_header = 'Question #2: \\nWhat information can you give us to reassure us that you are telling the truth?'\n",
    "\n",
    "    activity = activity_header + row['signevent'] + new_line\n",
    "    q1 = activity_description_header + new_line + response_1_heading + row['q1'] + new_line\n",
    "    q2 = activity_reassurance_header + new_line + response_2_heading + row['q2'] + new_line\n",
    "    return activity + q1 + q2\n",
    "\n",
    "def construct_outcome(row):\n",
    "    outcome = \"Is the response truthful or deceptive?\\n\"\n",
    "    return outcome  + row['outcome_class'] + new_line\n",
    "\n",
    "def construct_liwc_json(row):\n",
    "    pass\n",
    "\n",
    "def construct_few_shot_prompt(few_shot_df, infer_row):\n",
    "    # constructed as a list\n",
    "    prompt = []\n",
    "    prompt.append(intro)\n",
    "    \n",
    "    for _, row in few_shot_df.iterrows():\n",
    "        prompt.append(delimiter)\n",
    "        prompt.append(construct_activity_scenario(row))\n",
    "        prompt.append(liwc_header)\n",
    "        prompt.append(construct_liwc_attributes_json(row))\n",
    "        prompt.append(delimiter)\n",
    "        prompt.append(construct_outcome(row))\n",
    "        prompt.append(delimiter)\n",
    "        prompt.append(new_line)    \n",
    "        prompt.append(new_line)\n",
    "\n",
    "    prompt.append(delimiter)\n",
    "    prompt.append(construct_activity_scenario(infer_row))\n",
    "    prompt.append(liwc_header)\n",
    "    prompt.append(construct_liwc_attributes_json(row))\n",
    "    prompt.append(delimiter) \n",
    "    prompt.append(construct_outcome(infer_row)) # has to have a blank outcome to be filled by the llm\n",
    "    prompt.append(delimiter)\n",
    "\n",
    "\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_indices(df, total, exclude_list):\n",
    "    import random\n",
    "    rand_list = []\n",
    "    count = 0\n",
    "    while count < total:\n",
    "        rand_row = random.randrange(df.shape[0])\n",
    "        if rand_row not in exclude_list:\n",
    "            rand_list.append(rand_row)\n",
    "            count += 1\n",
    "    return rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test indices: [746, 1619, 579, 824, 1241, 869, 476, 1167, 662, 1456]\n"
     ]
    }
   ],
   "source": [
    "test_indices = create_test_indices(df, nb_test_samples, few_shot_list)  # exclude the ones in the few shot list\n",
    "# test_indices = [1435]\n",
    "print(f'test indices: {test_indices}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: 746 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "INDEX: 1619 GROUND TRUTH: deceptive, RESPONSE: deceptive - correct\n",
      "INDEX: 579 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "INDEX: 824 GROUND TRUTH: deceptive, RESPONSE: deceptive - correct\n",
      "INDEX: 1241 GROUND TRUTH: deceptive, RESPONSE: deceptive - correct\n",
      "INDEX: 869 GROUND TRUTH: deceptive, RESPONSE: truthful - wrong\n",
      "INDEX: 476 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "INDEX: 1167 GROUND TRUTH: deceptive, RESPONSE: deceptive - correct\n",
      "INDEX: 662 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "INDEX: 1456 GROUND TRUTH: deceptive, RESPONSE: deceptive - correct\n"
     ]
    }
   ],
   "source": [
    "y_ground_truth = []  # for computing F1-score\n",
    "y_predicted = []\n",
    "\n",
    "for index in test_indices:\n",
    "    infer_row = df.loc[index].copy()\n",
    "    # print(f'Inferring the `class_outcome` for:\\n{infer_row}')\n",
    "    ground_truth = 'truthful' if infer_row['outcome_class'] == 't' else 'deceptive'\n",
    "    # mask the `outcome_class` field since you want to predict it\n",
    "    infer_row['outcome_class'] = ''\n",
    "\n",
    "    # print(f'Original\\n:{df.loc[index]}')\n",
    "    # print(f'infer row\\n: {infer_row}')\n",
    "\n",
    "    prompt = construct_few_shot_prompt(random_truth_deceit_df, infer_row)\n",
    "    prompt = ''.join(prompt)\n",
    "    \n",
    "    # print(f'Prompt:\\n{prompt}')\n",
    "\n",
    "    response = get_chat_completion_with_backoff(\n",
    "        prompt=prompt,\n",
    "        model=MODEL,\n",
    "    )    \n",
    "        \n",
    "    print(f'INDEX: {index} GROUND TRUTH: {ground_truth}, RESPONSE: {response} - {\"wrong\" if ground_truth != response else \"correct\"}')\n",
    "    y_ground_truth.append(ground_truth)\n",
    "    y_predicted.append(response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1-score: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('Weighted F1-score:', f1_score(y_ground_truth, y_predicted, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyagi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
