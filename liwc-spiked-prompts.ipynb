{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiments with dataset in paper,\n",
    "Explainable Verbal Deception Detection using Transformers\n",
    "Loukas Ilias, Felix Soldner and Bennett Kleinberg\n",
    "uses LIWC-15\n",
    "\"\"\"\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # source the ~/.zshrc file\n",
    "\n",
    "# https://platform.openai.com/docs/guides/rate-limits/error-mitigation\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "# constants, until you change them ;-)\n",
    "new_line = '\\n'\n",
    "nb_test_samples = 20\n",
    "nb_few_shot_samples_of_each_class = 8 # truth and deception\n",
    "delimiter = '```\\n'\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "# MODEL = \"text-davinci-003\"\n",
    "# MODEL = \"gpt-4\"\n",
    "MODEL = \"gpt-4-1106-preview\" # input tokes 3x cheaper and output tokens 2x cheaper than GPT-4\n",
    "temperature = 1\n",
    "SEED = 123"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1640, 101)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv ('dataset/LIWC-15 Results - sign_events_data_statements - LIWC Analysis.csv')\n",
    "# simple EDA\n",
    "# print(df)\n",
    "# print(df.columns)\n",
    "print(f'shape: {df.shape}')  # should be 1640 x 6\n",
    "# df.head\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the LIWC markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1640, 101)\n",
      "Index(['signevent', 'q1', 'q2', 'unid', 'id', 'outcome_class', 'Segment', 'WC',\n",
      "       'Analytic', 'Clout',\n",
      "       ...\n",
      "       'Colon', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Quote', 'Apostro',\n",
      "       'Parenth', 'OtherP', 'Emoji'],\n",
      "      dtype='object', length=101)\n",
      "['AllPunc']\n",
      "['Analytic', 'Apostro', 'Authentic', 'Clout', 'Colon', 'Comma', 'Dash', 'Dic', 'Emoji', 'Exclam']\n",
      "['OtherP', 'Parenth', 'Period', 'QMark', 'Quote', 'Segment', 'SemiC', 'Sixltr', 'Tone', 'WC']\n",
      "['WPS', 'achieve', 'adj', 'adverb', 'affect', 'affiliation', 'anger', 'anx', 'article', 'assent']\n",
      "['auxverb', 'bio', 'body', 'cause', 'certain', 'cogproc', 'compare', 'conj', 'death', 'differ']\n",
      "['discrep', 'drives', 'family', 'feel', 'female', 'filler', 'focusfuture', 'focuspast', 'focuspresent', 'friend']\n",
      "['function', 'health', 'hear', 'home', 'i', 'id', 'informal', 'ingest', 'insight', 'interrog']\n",
      "['ipron', 'leisure', 'male', 'money', 'motion', 'negate', 'negemo', 'netspeak', 'nonflu', 'number']\n",
      "['outcome_class', 'percept', 'posemo', 'power', 'ppron', 'prep', 'pronoun', 'q1', 'q2', 'quant']\n",
      "['relativ', 'relig', 'reward', 'risk', 'sad', 'see', 'sexual', 'shehe', 'signevent', 'social']\n",
      "['space', 'swear', 'tentat', 'they', 'time', 'unid', 'verb', 'we', 'work', 'you']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "liwc_15 = pd.read_csv ('dataset/LIWC-15 Results - sign_events_data_statements - LIWC Analysis.csv')\n",
    "# simple EDA\n",
    "\n",
    "print(f'shape: {liwc_15.shape}')  # should be 1640, \n",
    "print(liwc_15.columns)\n",
    "cols = sorted(liwc_15.columns)\n",
    "nb_attrib_per_line = 10\n",
    "print_buf = []\n",
    "for i, attrib in enumerate(cols):\n",
    "    print_buf.append(attrib)\n",
    "    if i % nb_attrib_per_line == 0:\n",
    "        print(print_buf)\n",
    "        print_buf = []\n",
    "\n",
    "truth_markers = df[['ingest', 'bio', 'Analytic', 'number', 'leisure', 'focusfuture']]\n",
    "deception_markers = df[['Apostro', 'focuspast', 'reward', 'WC', 'pronoun', 'ppron', 'Exclam', 'Tone']]\n",
    "liwc_markers = df[['ingest', 'bio', 'Analytic', 'number', 'leisure', 'Apostro', 'focuspast', 'reward', 'WC', 'pronoun']]\n",
    "# print(truth_markers)\n",
    "# print(deception_markers)\n",
    "# print(liwc_markers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some quick test to see how the truth/deceit markers are bahaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signevent                                  My brothers wedding\n",
      "q1           My little brother is getting married next Satu...\n",
      "q2           My brother and Kate have a daughter who will b...\n",
      "unid                                                  FU304384\n",
      "id                                                           1\n",
      "                                   ...                        \n",
      "Quote                                                      0.0\n",
      "Apostro                                                    0.0\n",
      "Parenth                                                    0.0\n",
      "OtherP                                                     0.0\n",
      "Emoji                                                      0.0\n",
      "Name: 0, Length: 101, dtype: object\n",
      "truth attributes: {\"ingestion\": 0, \"biological processes\": 0, \"analytic reasoning\": 27, \"numbers\": 1, \"leisure\": 1, \"future focus\": 3}\n",
      "deception attributes: {\"apostrophes\": 0, \"past focus\": 4, \"reward\": 3, \"word count\": 113, \"all pronoun\": 15, \"personal pronouns\": 13, \"axclamation marks\": 0, \"emotional tone\": 94}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'outcome'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/babyagi/lib/python3.9/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/babyagi/lib/python3.9/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/babyagi/lib/python3.9/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'outcome'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruth attributes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruth_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeception attributes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeception_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcome: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutcome\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/babyagi/lib/python3.9/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/babyagi/lib/python3.9/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/anaconda3/envs/babyagi/lib/python3.9/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'outcome'"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import json\n",
    "# TODO see if this can be more friendlier if GPT does not know about LIWC\n",
    "def construct_liwc_attributes_json(row):\n",
    "    # print('class:', 'truthful' if df.iloc[row]['outcome_class'] == 't' else 'deceptive')\n",
    "    # print(liwc_markers.iloc[row])\n",
    "    # print(f'q1:\\n {textwrap.fill(df.iloc[row][\"q1\"], 100)}')\n",
    "    # print(f'q2:\\n {textwrap.fill(df.iloc[row][\"q2\"], 100)}')\n",
    "    \n",
    "    truthful_attributes = ['ingest', 'bio', 'Analytic', 'number', 'leisure', 'focusfuture']\n",
    "    deceptive_attributes = ['Apostro', 'focuspast', 'reward', 'WC', 'pronoun', 'ppron', 'Exclam', 'Tone']\n",
    "    \n",
    "    freindly_truthful_attribs = ['ingestion', 'biological processes', 'analytic reasoning', \n",
    "                                 'numbers', 'leisure', 'future focus']\n",
    "    freindly_deceptive_attribs = ['apostrophes', 'past focus', 'reward', 'word count', \n",
    "                                  'all pronoun', 'personal pronouns', 'axclamation marks', 'emotional tone']\n",
    "                  \n",
    "    # attributes = ['ingest', 'bio', 'Analytic', 'number', 'leisure', 'Apostro', 'focuspast', 'reward', 'WC', 'pronoun']\n",
    "    truthful_data = {}\n",
    "    for truthful_attribute, friendly_truthful_attrib in zip(truthful_attributes, freindly_truthful_attribs):\n",
    "        # data[attribute] = str(row[attribute])\n",
    "        truthful_data[friendly_truthful_attrib] = int(row[truthful_attribute]) \n",
    "    deceptive_data = {}\n",
    "    for deceptive_attribute, friendly_deceptive_attrib in zip(deceptive_attributes, freindly_deceptive_attribs):\n",
    "        # data[attribute] = str(row[attribute])\n",
    "        deceptive_data[friendly_deceptive_attrib] = int(row[deceptive_attribute]) \n",
    "        \n",
    "    \n",
    "    return json.dumps(truthful_data), json.dumps(deceptive_data)\n",
    "\n",
    "print(df.iloc[0])\n",
    "truth_json, deception_json = construct_liwc_attributes_json(df.iloc[0].copy())\n",
    "print(f'truth attributes: {truth_json}')\n",
    "print(f'deception attributes: {deception_json}')\n",
    "print(f\"outcome: {df.iloc[0]['outcome_class']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a random list of indices (from both classes) to be used for k-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth df shape: (783, 101)\n",
      "deceit df shape: (857, 101)\n",
      "non-repeating random numbers are:\n",
      "truth indices:\" [508, 708, 698, 536, 339, 454, 756, 452]\n",
      "non-repeating random numbers are:\n",
      "deceit indices: [1501, 954, 1380, 1495, 1071, 806, 1440, 1038]\n",
      "truth + deceit indices(few-shot-list)\n",
      " [508, 708, 698, 536, 339, 454, 756, 452, 1501, 954, 1380, 1495, 1071, 806, 1440, 1038]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36845/1262882511.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  truth_df['outcome_class'] = df['outcome_class'].replace('t','truthful')\n",
      "/tmp/ipykernel_36845/1262882511.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deceit_df['outcome_class'] = df['outcome_class'].replace('d','deceptive')\n"
     ]
    }
   ],
   "source": [
    "def filter_by_class(df, category):\n",
    "   return df[df['outcome_class']== category]\n",
    "\n",
    "truth_df = filter_by_class(df, 't')\n",
    "# print(truth_df)\n",
    "print(f'truth df shape: {truth_df.shape}')  # should be 1640 x 6\n",
    "\n",
    "# replace with a more expressive word, truthful\n",
    "truth_df['outcome_class'] = df['outcome_class'].replace('t','truthful')\n",
    "# print(truth_df)\n",
    "\n",
    "deceit_df = filter_by_class(df, 'd')\n",
    "# print(deceit_df)\n",
    "print(f'deceit df shape: {deceit_df.shape}')  # should be 1640 x 6\n",
    "\n",
    "# replace with a more expressive word, deceitful\n",
    "deceit_df['outcome_class'] = df['outcome_class'].replace('d','deceptive')\n",
    "# print(deceit_df)\n",
    "\n",
    "# pick random non-repeating rows\n",
    "def pick_randon_non_repeating(df, quantity):\n",
    "    import random\n",
    "    rand_df = pd.DataFrame()\n",
    "    random_list = random.sample(range(df.shape[0]), quantity)\n",
    "    print(\"non-repeating random numbers are:\")\n",
    "    return df.iloc[random_list], random_list\n",
    "\n",
    "random_truth_df, truth_indices_list = pick_randon_non_repeating(truth_df, nb_few_shot_samples_of_each_class)\n",
    "# print(f'random truth list:\\n {random_truth_df}')\n",
    "print(f'truth indices:\" {truth_indices_list}')\n",
    "\n",
    "random_deceit_df, deceit_indices_list = pick_randon_non_repeating(deceit_df, nb_few_shot_samples_of_each_class)\n",
    "# print(f'random deceit list:\\n {random_deceit_df}')\n",
    "deceit_indices_list = [x + truth_df.shape[0] for x in deceit_indices_list] # do this to exclude from original list\n",
    "print(f'deceit indices: {deceit_indices_list}')\n",
    "\n",
    "random_truth_deceit_df = pd.concat([random_truth_df, random_deceit_df])\n",
    "few_shot_list = truth_indices_list + deceit_indices_list\n",
    "print(f'truth + deceit indices(few-shot-list)\\n {few_shot_list}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the OpenAI call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(prompt, model, temperature):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=3), stop=stop_after_attempt(3))\n",
    "def get_chat_completion_with_backoff(prompt, model, temperature):\n",
    "    return get_chat_completion(prompt, model, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = f\"\"\"\n",
    "You are tasked to classify the response to questions into two classes: truthful or deceptive.\n",
    "You'll be presented with the following pieces of information on an activity:\n",
    "(1) The title of the activity.\n",
    "(2) An answer to the question: Please describe your activity. Be as specific as possible.\n",
    "(3) An answer to the follow-on question: What information can you give us to reassure us that you are telling the truth?\n",
    "(4) A Linguistic Inquiry Word Count (LIWC) category key and value as JSON data\n",
    "Using all four pieces of information, complete the response that must be either 'truthful' or 'deceptive' and nothig else.\n",
    "\n",
    "Here are a few examples delimited by triple backticks:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response_1_heading = \"\"\"Response #1:\\n\"\"\"\n",
    "response_2_heading = \"\"\"Response #2:\\n\"\"\"\n",
    "\n",
    "liwc_header = \"\"\"Linguistic Inquiry Word Count (LIWC) of responses in JSON key/value format:\\n\"\"\"\n",
    "\n",
    "def construct_activity_scenario(row):\n",
    "    # activity_header = 'Title of the Activity: ' + new_line\n",
    "    activity_header = 'Activity: ' + new_line\n",
    "    activity_description_header = 'Question #1: Please describe your activity. Be as specific as possible.'\n",
    "\n",
    "    activity_reassurance_header = 'Question #2: What information can you give us to reassure us that you are telling the truth?'\n",
    "\n",
    "    activity = activity_header + row['signevent'] + 2* new_line\n",
    "    q1_r1 = activity_description_header + new_line + response_1_heading + row['q1'] + 2 * new_line\n",
    "    q2_r2 = activity_reassurance_header + new_line + response_2_heading + row['q2'] + 2 * new_line\n",
    "    return activity + q1_r1 + q2_r2\n",
    "\n",
    "def construct_outcome(row):\n",
    "    outcome = \"Are the responses truthful or deceptive?\\n\"\n",
    "    return outcome  + row['outcome_class'] + new_line\n",
    "\n",
    "def construct_liwc_json(row):\n",
    "    pass\n",
    "\n",
    "def construct_few_shot_prompt(few_shot_df, infer_row):\n",
    "    # constructed as a list\n",
    "    prompt = []\n",
    "    prompt.append(intro)\n",
    "    \n",
    "    for _, row in few_shot_df.iterrows():\n",
    "        prompt.append(delimiter)\n",
    "        prompt.append(construct_activity_scenario(row))\n",
    "        prompt.append(liwc_header)\n",
    "        prompt.append(construct_liwc_attributes_json(row))\n",
    "        prompt.append(2 * new_line)\n",
    "        prompt.append(construct_outcome(row))\n",
    "        prompt.append(delimiter)\n",
    "        prompt.append(new_line * 2)    \n",
    "\n",
    "    prompt.append(delimiter)\n",
    "    prompt.append(construct_activity_scenario(infer_row))\n",
    "    prompt.append(liwc_header)\n",
    "    prompt.append(construct_liwc_attributes_json(row))\n",
    "    prompt.append(2 * new_line) \n",
    "    prompt.append(construct_outcome(infer_row)) # has to have a blank outcome to be filled by the LLM\n",
    "    prompt.append(delimiter)\n",
    "\n",
    "\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_indices(df, total, exclude_list):\n",
    "    import random\n",
    "    rand_list = []\n",
    "    count = 0\n",
    "    while count < total:\n",
    "        rand_row = random.randrange(df.shape[0])\n",
    "        if rand_row not in exclude_list:\n",
    "            rand_list.append(rand_row)\n",
    "            count += 1\n",
    "    return rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test indices: [809, 1070, 54, 1621, 876, 1327, 677, 963, 1280, 800, 361, 1239, 103, 41, 155, 254, 36, 1244, 710, 1281]\n"
     ]
    }
   ],
   "source": [
    "test_indices = create_test_indices(df, nb_test_samples, few_shot_list)  # exclude the ones in the few shot list\n",
    "print(f'test indices: {test_indices}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: INDEX: 809 GROUND TRUTH: deceptive, RESPONSE: deceptive - correct\n",
      "#1: INDEX: 1070 GROUND TRUTH: deceptive, RESPONSE: The responses cannot be classified definitively as truthful or deceptive solely based on the information given. While the Linguistic Inquiry Word Count (LIWC) analysis may provide some insight into the cognitive and emotional states communicated in the text, it is not a foolproof method to detect deception.\n",
      "\n",
      "The content of the responses, apparent consistency of details, and supporting information provided by individuals are factors that can contribute to the perception of truthfulness. However, these subjective interpretations cannot guarantee accuracy. Furthermore, the truthfulness of a response may be influenced by the individual's memory, communication skills, and intention.\n",
      "\n",
      "Therefore, without additional verification methods or more context-specific indicators of truthfulness or deception, it is not possible to conclusively classify the responses based on the provided information alone.\n",
      "\n",
      "Regarding the last example provided where a decision was not made, similar to the previous responses, it cannot be definitively classified as truthful or deceptive based on the given information. Additional evidence or verification would be needed to support a claim of truthfulness or deception. - wrong\n",
      "#2: INDEX: 54 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "#3: INDEX: 1621 GROUND TRUTH: deceptive, RESPONSE: truthful - wrong\n",
      "#4: INDEX: 876 GROUND TRUTH: deceptive, RESPONSE: deceptive - correct\n",
      "#5: INDEX: 1327 GROUND TRUTH: deceptive, RESPONSE: deceptive - correct\n",
      "#6: INDEX: 677 GROUND TRUTH: truthful, RESPONSE: truthful - correct\n",
      "#7: INDEX: 963 GROUND TRUTH: deceptive, RESPONSE: truthful - wrong\n",
      "#8: INDEX: 1280 GROUND TRUTH: deceptive, RESPONSE: Based on the examples provided and the methodology for the task, the examples that follow contain both specific and verifiable details and use concrete language - hallmarks that are often associated with truthfulness. However, in previous examples, even when specific details were provided, some classification results were marked as deceptive. Since the classification is imperfect without more context or external verification, any classification provided here is merely an educated guess based on the data presented, not a definitive judgment.\n",
      "\n",
      "Given the details in the provided responses for the Antique group weekend activity:\n",
      "\n",
      "```\n",
      "Activity: \n",
      "I'm going to Antique group weekend.\n",
      "\n",
      "Question #1: \n",
      "Please describe your activity. Be as specific as possible.\n",
      "Response #1:\n",
      "I found out about it in the online forum. I have already arranged the travel arrangements. Saturday morning I need to catch an early train to be there about 11. I am taking my family with me but they are not interested in the antiques as much as me. They will help me though to hunt for a hidden treasures. I love the history behind the objects and to be honest I couln't wait for this trip for a while now.\n",
      "\n",
      "Question #2: \n",
      "What information can you give us to reassure us that you are telling the truth?\n",
      "Response #2:\n",
      "I can give you the train time and details of all the arrangements I prepared, including the hotel name I'm staying in.\n",
      "\n",
      "Linguistic Inquiry Word Count (LIWC) of responses in JSON key/value format:\n",
      "{\"ingest\": 2, \"bio\": 3, \"Analytic\": 65, \"number\": 5, \"leisure\": 3, \"focusfuture\": 3, \"Apostro\": 7, \"focuspast\": 3, \"reward\": 1, \"WC\": 129, \"pronoun\": 14, \"ppron\": 10, \"Exclam\": 0, \"Tone\": 39}\n",
      "```\n",
      "\n",
      "The response exhibits specific and verifiable information, such as the mention of preparations, train times, and hotel details, which enhance its credibility. Furthermore, the LIWC values do not indicate obvious deception markers. However, the previous pattern of classifications suggests that specific details alone do not guarantee a classification of 'truthful'.\n",
      "\n",
      "Therefore, the classification can go either way without additional input or validation mechanisms, and so, in keeping with the established pattern, I would lean toward labeling this response as 'truthful', with the caveat that absolute certainty cannot be achieved without further evidence. - wrong\n",
      "#9: INDEX: 800 GROUND TRUTH: deceptive, RESPONSE: deceptive\n",
      " - wrong\n",
      "#10: INDEX: 361 GROUND TRUTH: truthful, RESPONSE: Are the responses truthful or deceptive?\n",
      "deceptive - wrong\n",
      "#11: INDEX: 1239 GROUND TRUTH: deceptive, RESPONSE: truthful - wrong\n",
      "#12: INDEX: 103 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "#13: INDEX: 41 GROUND TRUTH: truthful, RESPONSE: Regarding the last activity:\n",
      "\n",
      "Activity: \n",
      "My son will be getting married on Saturday the 28th so a big day all round for the whole family looking forward to it\n",
      "\n",
      "Question #1: \n",
      "Please describe your activity. Be as specific as possible.\n",
      "Response #1:\n",
      "My son will be getting married to his long-term girlfriend is two little girls will be flower girls so this will be really good his other two brothers will be handing out leaflets we think everything is all of together as long as the weather outside should be a good day\n",
      "\n",
      "Question #2: \n",
      "What information can you give us to reassure us that you are telling the truth?\n",
      "Response #2:\n",
      "They will be getting married on the 28th of July 2018 and it will be in Tintinhull church this is a village near Yeovil in Somerset then on to a reception in a stately house near South Petherton\n",
      "\n",
      "Linguistic Inquiry Word Count (LIWC) of responses in JSON key/value format:\n",
      "{\"ingest\": 2, \"bio\": 3, \"Analytic\": 65, \"number\": 5, \"leisure\": 3, \"focusfuture\": 3, \"Apostro\": 7, \"focuspast\": 3, \"reward\": 1, \"WC\": 129, \"pronoun\": 14, \"ppron\": 10, \"Exclam\": 0, \"Tone\": 39}\n",
      "\n",
      "Are the responses truthful or deceptive?\n",
      "truthful\n",
      "\n",
      "The provided responses seem detailed and consistent with the activity described. There is specific information about dates, locations, and activities during the wedding ceremony. Since the information seems plausible and no significant inconsistencies have been noted from the text provided, the responses can be classified as truthful. However, it should be noted that my ability to determine truthfulness or deception is limited to the text analysis and does not account for external validation or additional context that might affect the veracity of the statements. - wrong\n",
      "#14: INDEX: 155 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "#15: INDEX: 254 GROUND TRUTH: truthful, RESPONSE: truthful - correct\n",
      "#16: INDEX: 36 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "#17: INDEX: 1244 GROUND TRUTH: deceptive, RESPONSE: truthful - wrong\n",
      "#18: INDEX: 710 GROUND TRUTH: truthful, RESPONSE: deceptive - wrong\n",
      "#19: INDEX: 1281 GROUND TRUTH: deceptive, RESPONSE: truthful - wrong\n"
     ]
    }
   ],
   "source": [
    "y_ground_truth = []  # for computing F1-score\n",
    "y_predicted = []\n",
    "\n",
    "for i, index in enumerate(test_indices):\n",
    "    infer_row = df.loc[index].copy()\n",
    "    # print(f'Inferring the `class_outcome` for:\\n{infer_row}')\n",
    "    ground_truth = 'truthful' if infer_row['outcome_class'] == 't' else 'deceptive'\n",
    "    # mask the `outcome_class` field since you want to predict it\n",
    "    infer_row['outcome_class'] = ''\n",
    "\n",
    "    # print(f'Original\\n:{df.loc[index]}')\n",
    "    # print(f'infer row\\n: {infer_row}')\n",
    "\n",
    "    prompt = construct_few_shot_prompt(random_truth_deceit_df, infer_row)\n",
    "    prompt = ''.join(prompt)\n",
    "    \n",
    "    # print(f'Prompt:\\n{prompt}')\n",
    "\n",
    "    response = get_chat_completion_with_backoff(\n",
    "        prompt=prompt,\n",
    "        model=MODEL,\n",
    "        temperature=temperature\n",
    "    )    \n",
    "        \n",
    "    print(f'#{i}: INDEX: {index} GROUND TRUTH: {ground_truth}, RESPONSE: {response} - {\"wrong\" if ground_truth != response else \"correct\"}')\n",
    "    y_ground_truth.append(ground_truth)\n",
    "    y_predicted.append(response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples predicted: 20\n",
      "Weighted F1-score: 0.29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f'Total samples predicted: {len(y_predicted)}')\n",
    "print(f\"Weighted F1-score: {f1_score(y_ground_truth, y_predicted, average='weighted'):0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyagi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
