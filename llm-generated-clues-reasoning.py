"""
LLM generated CLUES and REASONING

Using dataset/sign_events_data_statements.csv with columns: 
signevent,q1,q2,unid,id,outcome_class
Dataset size: [1640 rows x 6 columns]
TRUTHFUL data [:782]
DECEPTIVE data [783:]

Adding the CLUES and REASONING columns generated by the LLM.
and calling it ['contains_clues', 'clues', 'reasoning']
"""

from openai_interface import init_openai, get_chat_completion_with_backoff
from llm_generated_clues_reason_output import prep_output_df, save_output_df, parse_n_write_response

init_openai()

dataset_used = 'dataset/sign_events_data_statements.csv'
llm_generated_dataset = 'dataset/llm_generated_clues_reasoning_events_data_statements.csv'

import pandas as pd
df = pd.read_csv (dataset_used)
# simple EDA
print(df.columns)
print(f'shape: {df.shape}')  # should be 1640 x 6
print(df.head)

newline = '\n'
# not puting any limit on the 'reasoning' word-count 

def create_prelude(gt):
    prelude = f"""
In the PARAGRAPH section below: 
First, highlight in the JSON format, words or phrases related to the TRUTHFUL and DECEPTIVE categories.
Treat each sub-category within the two categories in order of decreasing importance.
List each sub-category even if it it's empty.
Next, provide a detailed reason as to why the paragraph is {gt} based on what you find in the sub-categories below.
Finally, in one word, make a final classification on whether the paragraph is TRUTHFUL or DECEPTIVE. 
Generate the final response in the JSON format with keys, "TRUTHFUL", "DECEPTIVE", "REASONING", "CLASSIFICATION"

TRUTHFUL
ingestion - examples are: " dish", "eat", "pizza"
biological-processes - examples are: "eat", "blood", "pain"
numbers - examples are: "second", "thousand", "5", "10"
leisure - examples are: "cook", "chat", "movie" 
future-focus - examples are:  "may", "will", "soon"
DECEPTIVE
apostrophes - examples are, "haven't", "won't", "she's", "can't"
past-tense-focused -  examples are, "ago", "did", "talked", "promised", "gotten"
reward  - examples are, "congratulate", "accomplishment" , "take", "prize", "benefit"
pronouns - examples are, "I", "them", "itself"
personal-pronouns - examples are, "I", "them", "her"
exclamation-mark - example is, "!"
"""

    return prelude + newline

def create_input(row):
    """
    For now, conbine question 1 and question 2
    """
    return 'PARAGRAPH: ' + row['q1'] + '\n' + row['q2']

def construct_context(row, gt):
    """ 
    put the prelude and the response to q1 and q2 
    """
    context = ''
    # print(create_prelude())
    context += create_prelude(gt)
    context += create_input(df.loc[row].copy())
    return context


if __name__ == "__main__":
    ground_truths = []
    truth_start_row, truth_end_row = 0, 5
    deceptive_start_row, deceptive_end_row = 783, 788
    rows = list(range(truth_start_row, truth_end_row))
    rows += list(range(deceptive_start_row, deceptive_end_row))
    print(f'Rows used to generate the clues + reasoning: {rows}')
    out_df = prep_output_df(dataset_used, ['contains_clues', 'clues', 'reasoning'])

    model = 'gpt-4'  # "gpt-3.5-turbo" or "gpt-4"
    print(f'Model:{model}')
    for row in rows:
        print(f"{20*'-'}{row}{20*'-'}")
        ground_truth = 'truthful' if df.loc[row]['outcome_class'] == 't' else 'deceptive'
        final_context = construct_context(row=row, gt=ground_truth)
        print(final_context)
        print(f'ground truth (GT): {ground_truth}')
        # print(f'INPUT:\n Q1:\n {df.loc[row]["q1"]} \n Q2:\n {df.loc[row]["q2"]}')

        response = get_chat_completion_with_backoff(final_context, model=model)
        print(f'Response---------------------\n')
        print(response + newline)
        out_df = parse_n_write_response(response, out_df, row)

save_output_df(llm_generated_dataset, out_df)

